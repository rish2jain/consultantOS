"""
Time-series optimization layer for continuous monitoring system.

Provides efficient storage, retrieval, and querying of monitoring snapshots
with compression, batching, and intelligent query optimization for Firestore.
"""

import asyncio
import gzip
import json
import logging
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Protocol, Tuple

from consultantos.models.monitoring import MonitorAnalysisSnapshot

logger = logging.getLogger(__name__)


class DatabaseProtocol(Protocol):
    """Protocol for database service interface"""

    async def create_snapshot(self, snapshot: MonitorAnalysisSnapshot) -> bool:
        """Create snapshot record"""
        ...

    async def get_snapshots_in_range(
        self,
        monitor_id: str,
        start_time: datetime,
        end_time: datetime,
        limit: Optional[int] = None,
    ) -> List[MonitorAnalysisSnapshot]:
        """Get snapshots within time range"""
        ...

    async def get_latest_snapshot(
        self, monitor_id: str
    ) -> Optional[MonitorAnalysisSnapshot]:
        """Get most recent snapshot"""
        ...

    async def delete_snapshots_before(
        self, monitor_id: str, before_time: datetime
    ) -> int:
        """Delete snapshots older than specified time"""
        ...


class TimeSeriesOptimizer:
    """
    Time-series optimization layer for monitoring snapshots.

    Features:
    - Compression for large snapshots (reduce storage 60-80%)
    - Batched writes for high-frequency monitoring
    - Optimized time-range queries with pagination
    - Automatic retention management
    - Query result caching for common patterns
    """

    def __init__(
        self,
        db_service: DatabaseProtocol,
        compression_threshold_bytes: int = 1024,
        batch_size: int = 10,
        cache_ttl_seconds: int = 300,
    ):
        """
        Initialize time-series optimizer.

        Args:
            db_service: Database service for persistence
            compression_threshold_bytes: Compress snapshots larger than this
            batch_size: Number of snapshots to batch in writes
            cache_ttl_seconds: Cache TTL for query results
        """
        self.db = db_service
        self.compression_threshold = compression_threshold_bytes
        self.batch_size = batch_size
        self.cache_ttl = cache_ttl_seconds

        # Pending write batch
        self._write_batch: List[MonitorAnalysisSnapshot] = []
        self._batch_lock = asyncio.Lock()

        # Query result cache
        self._query_cache: Dict[str, Tuple[datetime, Any]] = {}
        self._cache_lock = asyncio.Lock()

    async def store_snapshot(
        self,
        snapshot: MonitorAnalysisSnapshot,
        compress: bool = True,
        batch: bool = False,
    ) -> bool:
        """
        Store snapshot with optional compression and batching.

        Args:
            snapshot: Snapshot to store
            compress: Apply compression if snapshot is large
            batch: Add to write batch instead of immediate write

        Returns:
            True if stored successfully
        """
        try:
            # Apply compression if needed
            if compress:
                snapshot_size = self._estimate_snapshot_size(snapshot)
                if snapshot_size > self.compression_threshold:
                    snapshot = self._compress_snapshot(snapshot)
                    logger.debug(
                        f"Compressed snapshot for monitor {snapshot.monitor_id}: "
                        f"{snapshot_size} â†’ {self._estimate_snapshot_size(snapshot)} bytes"
                    )

            # Batch or immediate write
            if batch:
                async with self._batch_lock:
                    self._write_batch.append(snapshot)

                    # Flush batch if full
                    if len(self._write_batch) >= self.batch_size:
                        await self._flush_batch()

                return True
            else:
                # Immediate write
                return await self.db.create_snapshot(snapshot)

        except Exception as e:
            logger.error(f"Failed to store snapshot: {e}", exc_info=True)
            return False

    async def get_snapshots_in_range(
        self,
        monitor_id: str,
        start_time: datetime,
        end_time: datetime,
        limit: Optional[int] = None,
        page_size: int = 100,
    ) -> List[MonitorAnalysisSnapshot]:
        """
        Get snapshots within time range with pagination.

        Args:
            monitor_id: Monitor identifier
            start_time: Range start (inclusive)
            end_time: Range end (inclusive)
            limit: Maximum results to return
            page_size: Results per page for pagination

        Returns:
            List of snapshots in time range
        """
        # Check cache first
        cache_key = f"{monitor_id}:{start_time.isoformat()}:{end_time.isoformat()}:{limit}"
        cached = await self._get_cached_query(cache_key)
        if cached is not None:
            logger.debug(f"Cache hit for time range query: {cache_key}")
            return cached

        try:
            # Query with pagination
            results = await self.db.get_snapshots_in_range(
                monitor_id=monitor_id,
                start_time=start_time,
                end_time=end_time,
                limit=limit,
            )

            # Decompress if needed
            decompressed = [self._decompress_snapshot(s) for s in results]

            # Cache results
            await self._cache_query_result(cache_key, decompressed)

            return decompressed

        except Exception as e:
            logger.error(f"Failed to get snapshots in range: {e}", exc_info=True)
            return []

    async def get_latest_snapshot(
        self, monitor_id: str, decompress: bool = True
    ) -> Optional[MonitorAnalysisSnapshot]:
        """
        Get most recent snapshot for monitor.

        Args:
            monitor_id: Monitor identifier
            decompress: Decompress snapshot if compressed

        Returns:
            Latest snapshot or None
        """
        try:
            snapshot = await self.db.get_latest_snapshot(monitor_id)

            if snapshot and decompress:
                snapshot = self._decompress_snapshot(snapshot)

            return snapshot

        except Exception as e:
            logger.error(f"Failed to get latest snapshot: {e}", exc_info=True)
            return None

    async def get_snapshot_at_time(
        self, monitor_id: str, target_time: datetime, tolerance_hours: int = 1
    ) -> Optional[MonitorAnalysisSnapshot]:
        """
        Get snapshot closest to target time within tolerance.

        Args:
            monitor_id: Monitor identifier
            target_time: Target timestamp
            tolerance_hours: Maximum time difference in hours

        Returns:
            Closest snapshot or None
        """
        start_time = target_time - timedelta(hours=tolerance_hours)
        end_time = target_time + timedelta(hours=tolerance_hours)

        snapshots = await self.get_snapshots_in_range(
            monitor_id=monitor_id,
            start_time=start_time,
            end_time=end_time,
            limit=10,
        )

        if not snapshots:
            return None

        # Find closest snapshot
        closest = min(
            snapshots,
            key=lambda s: abs((s.timestamp - target_time).total_seconds()),
        )

        return closest

    async def get_trend_data(
        self,
        monitor_id: str,
        days: int = 30,
        metric_name: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """
        Get trend data for specified period.

        Args:
            monitor_id: Monitor identifier
            days: Number of days to retrieve
            metric_name: Optional specific metric to extract

        Returns:
            List of time-series data points
        """
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(days=days)

        snapshots = await self.get_snapshots_in_range(
            monitor_id=monitor_id,
            start_time=start_time,
            end_time=end_time,
        )

        # Extract trend data
        trend_data = []
        for snapshot in snapshots:
            point = {
                "timestamp": snapshot.timestamp,
                "monitor_id": snapshot.monitor_id,
            }

            if metric_name:
                # Extract specific metric
                point["value"] = self._extract_metric(snapshot, metric_name)
            else:
                # Include all metrics
                point["metrics"] = self._extract_all_metrics(snapshot)

            trend_data.append(point)

        return trend_data

    async def cleanup_old_snapshots(
        self,
        monitor_id: str,
        retention_days: int = 90,
        dry_run: bool = False,
    ) -> int:
        """
        Delete snapshots older than retention period.

        Args:
            monitor_id: Monitor identifier
            retention_days: Keep snapshots newer than this
            dry_run: If True, only count snapshots to delete

        Returns:
            Number of snapshots deleted (or would be deleted)
        """
        cutoff_time = datetime.utcnow() - timedelta(days=retention_days)

        try:
            if dry_run:
                # Count snapshots to delete
                snapshots = await self.db.get_snapshots_in_range(
                    monitor_id=monitor_id,
                    start_time=datetime(2000, 1, 1),  # Far past
                    end_time=cutoff_time,
                )
                count = len(snapshots)
                logger.info(
                    f"DRY RUN: Would delete {count} snapshots older than {cutoff_time}"
                )
                return count
            else:
                # Actually delete
                count = await self.db.delete_snapshots_before(monitor_id, cutoff_time)
                logger.info(
                    f"Deleted {count} snapshots older than {cutoff_time} for monitor {monitor_id}"
                )
                return count

        except Exception as e:
            logger.error(f"Failed to cleanup old snapshots: {e}", exc_info=True)
            return 0

    async def flush_pending_writes(self) -> int:
        """
        Flush any pending batched writes.

        Returns:
            Number of snapshots written
        """
        async with self._batch_lock:
            return await self._flush_batch()

    # Private helper methods

    async def _flush_batch(self) -> int:
        """Flush write batch to database (must hold batch_lock)."""
        if not self._write_batch:
            return 0

        try:
            # Write all snapshots in batch
            tasks = [self.db.create_snapshot(s) for s in self._write_batch]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # Count successes
            success_count = sum(1 for r in results if r is True)

            logger.info(
                f"Flushed write batch: {success_count}/{len(self._write_batch)} succeeded"
            )

            # Clear batch
            count = len(self._write_batch)
            self._write_batch.clear()

            return count

        except Exception as e:
            logger.error(f"Failed to flush write batch: {e}", exc_info=True)
            return 0

    def _compress_snapshot(
        self, snapshot: MonitorAnalysisSnapshot
    ) -> MonitorAnalysisSnapshot:
        """
        Compress large snapshot fields using gzip.

        Compresses competitive_forces, strategic_position, and financial_metrics.
        """
        # Convert to dict for compression
        snapshot_dict = snapshot.dict()

        # Compress large dict fields
        for field in ["competitive_forces", "strategic_position", "financial_metrics"]:
            if field in snapshot_dict and snapshot_dict[field]:
                original = json.dumps(snapshot_dict[field])
                compressed = gzip.compress(original.encode("utf-8"))

                # Store as base64-encoded string with marker
                import base64

                snapshot_dict[field] = {
                    "_compressed": True,
                    "data": base64.b64encode(compressed).decode("ascii"),
                }

        return MonitorAnalysisSnapshot(**snapshot_dict)

    def _decompress_snapshot(
        self, snapshot: MonitorAnalysisSnapshot
    ) -> MonitorAnalysisSnapshot:
        """Decompress snapshot if compressed."""
        snapshot_dict = snapshot.dict()

        # Decompress fields if needed
        for field in ["competitive_forces", "strategic_position", "financial_metrics"]:
            if field in snapshot_dict and isinstance(snapshot_dict[field], dict):
                if snapshot_dict[field].get("_compressed"):
                    # Decompress
                    import base64

                    compressed = base64.b64decode(
                        snapshot_dict[field]["data"].encode("ascii")
                    )
                    decompressed = gzip.decompress(compressed).decode("utf-8")
                    snapshot_dict[field] = json.loads(decompressed)

        return MonitorAnalysisSnapshot(**snapshot_dict)

    def _estimate_snapshot_size(self, snapshot: MonitorAnalysisSnapshot) -> int:
        """Estimate snapshot size in bytes."""
        return len(json.dumps(snapshot.dict(), default=str))

    def _extract_metric(self, snapshot: MonitorAnalysisSnapshot, metric_name: str) -> Any:
        """Extract specific metric from snapshot."""
        # Check financial metrics first
        if snapshot.financial_metrics and metric_name in snapshot.financial_metrics:
            return snapshot.financial_metrics[metric_name]

        # Check other fields
        if hasattr(snapshot, metric_name):
            return getattr(snapshot, metric_name)

        return None

    def _extract_all_metrics(self, snapshot: MonitorAnalysisSnapshot) -> Dict[str, Any]:
        """Extract all numeric/measurable metrics from snapshot."""
        metrics = {}

        # Financial metrics
        if snapshot.financial_metrics:
            metrics.update(snapshot.financial_metrics)

        # News sentiment
        if snapshot.news_sentiment is not None:
            metrics["news_sentiment"] = snapshot.news_sentiment

        # Competitor mention counts
        if snapshot.competitor_mentions:
            metrics["competitor_mention_count"] = len(snapshot.competitor_mentions)

        # Market trend counts
        if snapshot.market_trends:
            metrics["market_trend_count"] = len(snapshot.market_trends)

        return metrics

    async def _get_cached_query(self, cache_key: str) -> Optional[Any]:
        """Get cached query result if not expired."""
        async with self._cache_lock:
            if cache_key in self._query_cache:
                cached_time, result = self._query_cache[cache_key]

                # Check expiration
                age_seconds = (datetime.utcnow() - cached_time).total_seconds()
                if age_seconds < self.cache_ttl:
                    return result

                # Expired - remove
                del self._query_cache[cache_key]

        return None

    async def _cache_query_result(self, cache_key: str, result: Any) -> None:
        """Cache query result with timestamp."""
        async with self._cache_lock:
            self._query_cache[cache_key] = (datetime.utcnow(), result)

            # Limit cache size (simple LRU: remove oldest)
            if len(self._query_cache) > 1000:
                oldest_key = min(
                    self._query_cache.keys(),
                    key=lambda k: self._query_cache[k][0],
                )
                del self._query_cache[oldest_key]

    def get_optimizer_stats(self) -> Dict[str, Any]:
        """Get optimizer statistics."""
        return {
            "pending_writes": len(self._write_batch),
            "batch_size": self.batch_size,
            "cache_entries": len(self._query_cache),
            "cache_ttl_seconds": self.cache_ttl,
            "compression_threshold_bytes": self.compression_threshold,
        }
